{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/04%20-%20Preprocessing%20del%20testo/codifica_testo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00dbcadfetQ6"
   },
   "source": [
    "# Codifica del testo\n",
    "In questo notebook vedremo come implementare le due tecniche principali per la codifica del testo in numeri, **Bag of Words** e **TF-IDF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PI6A9DW7ev_8"
   },
   "source": [
    "## Bag of Words\n",
    "Il modello Bag of Words consiste nel creare un vocabolario di tutte le parole presenti all'interno del nostro corpus di testo, per poi contare quante volte ognuna di esse compare all'interno di un documento. Dato che un singolo documento contiene solo una minuscola parte dell'intero vocabolario, il risultato del bag of words sarà una matrice sparsa, cioè una matrice composta per lo più da zeri.\n",
    "<br>\n",
    "Definiamo un piccolo corpus di testo che useremo per un'esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkS9h37glx9t"
   },
   "outputs": [],
   "source": [
    "corpus = [\"la mamma prepara la pasta\", \"la nonna prepara la pizza\", \"papà guarda la mamma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cj3fZzQhwOSr"
   },
   "source": [
    "Definiamo una funzione che estrae i tokens da ogni documento all'interno del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXxwHSfWwULt"
   },
   "outputs": [],
   "source": [
    "def corpus_tokenizer(corpus):\n",
    "  tokens = [sent.split() for sent in corpus] \n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['la', 'mamma', 'prepara', 'la', 'pasta'],\n",
       " ['la', 'nonna', 'prepara', 'la', 'pizza'],\n",
       " ['papà', 'guarda', 'la', 'mamma']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token con espressioni regolari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkS9h37glx9t"
   },
   "outputs": [],
   "source": [
    "corpus = [\"la mamma, prepara la pasta\", \"la nonna prepara, la pizza\", \"Papà! Guarda la partita\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_tokenizer(corpus):\n",
    "    tokens = [re.split(\"\\W+\", sent.lower()) for sent in corpus]\n",
    "    return tokens                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = corpus_tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3khXOBc2wWCL"
   },
   "source": [
    "Definiamo una funzione che creerà il dizionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicizzazione parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1k2ZjoEeQWU"
   },
   "outputs": [],
   "source": [
    "def build_vocab(corpus_tokens):\n",
    "  \n",
    "  # usando un set evitiamo\n",
    "  # l'inserimento di duplicati\n",
    "  vocab = set({})\n",
    "  \n",
    "  for tokens in corpus_tokens:\n",
    "    for token in tokens:\n",
    "      vocab.add(token)\n",
    "    \n",
    "  return list(vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pasta', 'prepara', 'pizza', 'nonna', 'la', 'papà', 'guarda', 'partita', 'mamma']\n"
     ]
    }
   ],
   "source": [
    "index_to_word = build_vocab (corpus_tokens)\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nonna'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = dict([word, i] for i, word in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"nonna\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwfSJfQiwnKM"
   },
   "source": [
    "Adesso definiamo la funzione per eseguire il bag of words, la matrice risultate avrà un numero di righe pari al numero di documenti e un numero di colonne pari al numero di parole all'interno del vocabolario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONJeQQ0GfRVl"
   },
   "outputs": [],
   "source": [
    "def bag_of_words(corpus):\n",
    "\n",
    "  corpus_tokens = corpus_tokenizer(corpus)\n",
    "  vocab = build_vocab(corpus_tokens)\n",
    "  \n",
    "  corpus_bow = []\n",
    "\n",
    "  for tokens in corpus_tokens:\n",
    "    sent_bow = []\n",
    "  \n",
    "    for word in vocab:\n",
    "      sent_bow.append(tokens.count(word))\n",
    "    corpus_bow.append(sent_bow)\n",
    "  \n",
    "  return corpus_bow, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "MRXI5ybUl483",
    "outputId": "8a109100-aa73-4479-ddf7-9b2c2e37825f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dizionario: ['mamma', 'nonna', 'prepara', 'pizza', 'papà', 'pasta', 'guarda', 'la']\n",
      "\n",
      "\n",
      "Frase: la mamma prepara la pasta\n",
      "Bag of Words: [1, 0, 1, 0, 0, 1, 0, 2]\n",
      "-------\n",
      "Frase: la nonna prepara la pizza\n",
      "Bag of Words: [0, 1, 1, 1, 0, 0, 0, 2]\n",
      "-------\n",
      "Frase: papà guarda la mamma\n",
      "Bag of Words: [1, 0, 0, 0, 1, 0, 1, 1]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "corpus_bow, vocab = bag_of_words(corpus)\n",
    "\n",
    "print(\"Dizionario:\",vocab)\n",
    "print(\"\\n\")\n",
    "\n",
    "for sent, bow in zip(corpus, corpus_bow):\n",
    "  print(\"Frase:\", sent)\n",
    "  print(\"Bag of Words:\", bow)\n",
    "  print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cq2CsYsznpo"
   },
   "source": [
    "## Bag of Words con Numpy\n",
    "Vediamo ora un'implementazione del bag of words che sfrutta numpy, importiamo la libreria, in questo caso abbiamo creato un vocabolario inverso che mappa una parola al corrispondente indice all'interno del vocabolario tramite un dizionario, così facendo possiamo eseguire la codifica iterando sui token piuttosto che su tutte le parole del vocabolario. Questa tecnica porta un risparmio di tempo notevole quando il vocabolario è di grandi dimensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_LuyXFHzs25"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bag_of_words_np(corpus):\n",
    "\n",
    "  corpus_tokens = corpus_tokenizer(corpus)\n",
    "  \n",
    "  index_to_word = build_vocab(corpus_tokens)\n",
    "  word_to_index = dict([(char, i) for i, char in enumerate(index_to_word)]) # creiamo il vocabolario inverso\n",
    "  \n",
    "  docs_count = len(corpus)\n",
    "  vocab_size = len(index_to_word)\n",
    "  \n",
    "  corpus_bow = np.zeros((docs_count, vocab_size)) #numero di frasi come riga, numero di parole come colonne\n",
    "\n",
    "  for i, tokens in enumerate(corpus_tokens):\n",
    "    for token in tokens:\n",
    "      corpus_bow[i][word_to_index[token]]+=1 # usiamo il vocabolario inverso\n",
    "  \n",
    "  return corpus_bow, index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1obNyd22ErB"
   },
   "source": [
    "Testiamo la funzione, come vedi il risultato è lo stesso di prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "UbvszSfA0mgn",
    "outputId": "36f01344-9e92-42ad-ee14-15a1c2b0afe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dizionario: ['pasta', 'prepara', 'pizza', 'nonna', 'la', 'papà', 'guarda', 'partita', 'mamma']\n",
      "\n",
      "\n",
      "Frase: la mamma, prepara la pasta\n",
      "Bag of Words: [1. 1. 0. 0. 2. 0. 0. 0. 1.]\n",
      "-------\n",
      "Frase: la nonna prepara, la pizza\n",
      "Bag of Words: [0. 1. 1. 1. 2. 0. 0. 0. 0.]\n",
      "-------\n",
      "Frase: Papà! Guarda la partita\n",
      "Bag of Words: [0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "corpus_bow, vocab = bag_of_words_np(corpus)\n",
    "\n",
    "print(\"Dizionario:\",vocab)\n",
    "print(\"\\n\")\n",
    "\n",
    "for sent, bow in zip(corpus, corpus_bow): #zip ci permette di eseguire un ciclo in simultanea su due liste\n",
    "  print(\"Frase:\", sent)\n",
    "  print(\"Bag of Words:\", bow)\n",
    "  print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDVYEp5nhCrP"
   },
   "source": [
    "## TF-IDF\n",
    "Il TF-IDF, abbreviazione di Term Frequency - Inverse Document Frequency è un modello simile al bag of words, ma che tende a penalizzare le parole comuni all'interno del corpus di testo e a dare maggiore importanza a quelle più rare.\n",
    "<br>\n",
    "Il TF-IDF è composto dai seguenti elementi:\n",
    "- **Term Frequency**: quante volte un termine appare all'interno di un documento.\n",
    "- **Document Frequency**: in quanti documenti appare un termine.\n",
    "- **Inverse Document Frequency**: è il logaritmo del rapporto tra il numero di documenti e il document frequency, più uno se vogliamo evitare che le parole presenti in ogni documento abbiamo un IDF di 0.\n",
    "\n",
    "Il TF-IDF è dato dal prodotto del Term Frequency e l'Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYi2qVJrnvYD"
   },
   "outputs": [],
   "source": [
    "corpus = [\"la mamma prepara la pasta\", \"la nonna prepara la pizza\", \"papà guarda la mamma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "im6FL2cmihsL"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def tf_idf(corpus):\n",
    "  \n",
    "  corpus_tokens = corpus_tokenizer(corpus)\n",
    "  vocab = build_vocab(corpus_tokens)\n",
    "\n",
    "  # Document Frequency\n",
    "    \n",
    "  df = [0]*len(vocab)\n",
    "\n",
    "  for i,word in enumerate(vocab):\n",
    "    for tokens in corpus_tokens:\n",
    "      if(word in tokens):\n",
    "        df[i]+=1\n",
    "\n",
    "  # Inverse Document frequency\n",
    "        \n",
    "  docs_count = len(corpus)\n",
    "  idf = [log(docs_count/i)+1 for i in df]\n",
    "\n",
    "  # Term Frequency\n",
    "  \n",
    "  tf = []\n",
    "\n",
    "  for tokens in corpus_tokens:\n",
    "    tf_sent = []\n",
    "    words_count = len(tokens)\n",
    "  \n",
    "    for word in vocab:\n",
    "      tf_sent.append(tokens.count(word)/words_count)\n",
    "    tf.append(tf_sent)\n",
    "\n",
    "  #TF-IDF\n",
    "    \n",
    "  tf_idf = []\n",
    "\n",
    "  for i in range(docs_count):\n",
    "    tf_idf.append([tf_i*idf_i for tf_i,idf_i in zip(tf[i],idf)])\n",
    "\n",
    "  return tf_idf, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ADfjwM33d9-"
   },
   "source": [
    "Testiamo la nostra implementazione del TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "2Pn-dpVPmjYg",
    "outputId": "b0154df4-95ed-4cc3-a8e3-d42af8556905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dizionario: ['prepara', 'guarda', 'pasta', 'la', 'nonna', 'mamma', 'pizza', 'papà']\n",
      "\n",
      "\n",
      "Frase: la mamma prepara la pasta\n",
      "TF-IDF: [0.2810930216216329, 0.0, 0.41972245773362205, 0.4, 0.0, 0.2810930216216329, 0.0, 0.0]\n",
      "-------\n",
      "Frase: la nonna prepara la pizza\n",
      "TF-IDF: [0.2810930216216329, 0.0, 0.0, 0.4, 0.41972245773362205, 0.0, 0.41972245773362205, 0.0]\n",
      "-------\n",
      "Frase: papà guarda la mamma\n",
      "TF-IDF: [0.0, 0.5246530721670275, 0.0, 0.25, 0.0, 0.3513662770270411, 0.0, 0.5246530721670275]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf, vocab = tf_idf(corpus)\n",
    "\n",
    "print(\"Dizionario:\",vocab)\n",
    "print(\"\\n\")\n",
    "\n",
    "for sent, tfidf in zip(corpus, corpus_tfidf):\n",
    "  print(\"Frase:\", sent)\n",
    "  print(\"TF-IDF:\", tfidf)\n",
    "  print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqU5cFK6qdUc"
   },
   "source": [
    "##TF-IDF con Numpy\n",
    "Implementiamo il TF-IDF nuovamente con Numpy, utilizziamo anche in questo caso la tecnica del vocabolario inverso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9_flwYHpJf9"
   },
   "outputs": [],
   "source": [
    "def np_tf_idf(corpus):\n",
    "  \n",
    "  corpus_tokens = corpus_tokenizer(corpus)\n",
    "  \n",
    "  index_to_word = build_vocab(corpus_tokens)\n",
    "  word_to_index = dict([(char, i) for i, char in enumerate(index_to_word)]) # creiamo il vocabolario inverso\n",
    "  \n",
    "  vocab_size = len(index_to_word)\n",
    "  docs_count = len(corpus)\n",
    "  \n",
    "  # Document Frequency\n",
    "    \n",
    "  df = np.zeros(vocab_size)\n",
    "\n",
    "  for i,word in enumerate(index_to_word):\n",
    "    for tokens in corpus_tokens:\n",
    "      if(word in tokens):\n",
    "        df[i]+=1\n",
    "\n",
    "  # Inverse Document frequency\n",
    "        \n",
    "  idf = np.log(docs_count/df)+1\n",
    "\n",
    "  # Term Frequency\n",
    "  \n",
    "  tf = np.zeros((docs_count, vocab_size))\n",
    "  \n",
    "  for i, tokens in enumerate(corpus_tokens):\n",
    "    word_counts = len(tokens)\n",
    "    for token in tokens:\n",
    "      tf[i][word_to_index[token]]+=1 # usiamo il vocabolario inverso\n",
    "    tf[i]/=word_counts\n",
    "\n",
    "  #TF-IDF\n",
    "    \n",
    "  tf_idf = tf*idf\n",
    "\n",
    "  return tf_idf, index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KU8wVfUm5Nmb"
   },
   "source": [
    "Testiamo la nostra nuova implementazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "ohwqO7JitMzc",
    "outputId": "43accc17-e7bb-4c92-a013-fdf62e155b51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dizionario: ['prepara', 'guarda', 'pasta', 'la', 'nonna', 'mamma', 'pizza', 'papà']\n",
      "\n",
      "\n",
      "Frase: la mamma prepara la pasta\n",
      "TF-IDF: [0.28109302 0.         0.41972246 0.4        0.         0.28109302\n",
      " 0.         0.        ]\n",
      "-------\n",
      "Frase: la nonna prepara la pizza\n",
      "TF-IDF: [0.28109302 0.         0.         0.4        0.41972246 0.\n",
      " 0.41972246 0.        ]\n",
      "-------\n",
      "Frase: papà guarda la mamma\n",
      "TF-IDF: [0.         0.52465307 0.         0.25       0.         0.35136628\n",
      " 0.         0.52465307]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf, vocab = np_tf_idf(corpus)\n",
    "\n",
    "print(\"Dizionario:\",vocab)\n",
    "print(\"\\n\")\n",
    "\n",
    "for sent, tfidf in zip(corpus, corpus_tfidf):\n",
    "  print(\"Frase:\", sent)\n",
    "  print(\"TF-IDF:\", tfidf)\n",
    "  print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yg-U-7Ty5PzB"
   },
   "source": [
    "Il risultato è lo stesso di prima, ma questa implementazione performa meglio nel caso di corpus di testo molto grandi."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "codifica_testo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
