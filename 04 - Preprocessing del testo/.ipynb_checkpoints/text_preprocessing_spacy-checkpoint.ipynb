{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/4%20-%20Preprocessing%20del%20testo/text_preprocessing_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9lp6Cf0pa2M"
   },
   "source": [
    "# Preprocessing del testo con Spacy\n",
    "\n",
    "In questo notebook vedremo come possiamo eseguire la tokenizzazione, la rimozione delle stopwords e la lemmatizzazione con [Spacy](https://spacy.io/), una libreria Python per il natural language processing ad alte prestazioni progettata per essere utilizzata in produzione.\n",
    "Spacy adotta una politica del tipo \"pochi ma buoni\", cioè implementa solamente la tecnica più peformante per ogni tipo di operazione, per questo motivo spacy non implementa una funzione per lo stemming dato che queste solitamente portano a risultati più scarsi rispetto alla lemmatizzazione. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0MMYiLYt-kH"
   },
   "source": [
    "## Caricare un modello linguistico\n",
    "Per utilizzare Spacy dobbiamo prima scaricare e installare il modello per la lingua che vogliamo utilizzare, Spacy supporta oltre 49 lingue, [qui puoi trovare l'elenco completo con i relativi modelli](https://spacy.io/usage/models). Scarichiamo ed installiamo il modello per la lingua inglese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp39-cp39-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.2-cp39-cp39-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 846 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp39-cp39-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 183 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp39-cp39-macosx_10_9_x86_64.whl (460 kB)\n",
      "\u001b[K     |████████████████████████████████| 460 kB 120 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.20.3)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp39-cp39-macosx_10_9_x86_64.whl (783 kB)\n",
      "\u001b[K     |████████████████████████████████| 783 kB 969 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 339 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 81 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp39-cp39-macosx_10_9_x86_64.whl (18 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 53 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp39-cp39-macosx_10_9_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 47 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.8 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 pydantic-1.9.2 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.0 typer-0.4.2 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 170 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.62.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.20.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm #installation of english package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-gM65Yu2pWdB",
    "outputId": "8b1bf78d-5ca5-49f2-89c6-7ddb5cad2de6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") #english\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZC5qfmCu0W_"
   },
   "source": [
    "L'output della funzione load è un'oggetto di tipo *Language*, che possiamo utilizzare per processare il nostro testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ry8GhJMAum-D",
    "outputId": "402bc671-6816-4c0e-b16c-210518de1f9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"That's was a great play by you. I hope to play with you again.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5Ej3mQ7vmWN"
   },
   "source": [
    "doc è un oggetto contentente il testo già processato, cosa vuol dire questo ? Che operazioni come tokenizzazione e lemmatizzazione sono già state eseguite e possiamo utilizzare degli appositi attributi per accedere ai risultati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqIaTSlyv7rf"
   },
   "source": [
    "## Tokenizzazione\n",
    "Possiamo accedere ai singoli token utilizzando l'attributo **.text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "That"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "jdpkGGOnwA0Y",
    "outputId": "12baea65-575a-4730-e66d-1e3cd32676ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primo token: That\n",
      "Ultimo token: .\n",
      "\n",
      "\n",
      "That\n",
      "'s\n",
      "was\n",
      "a\n",
      "great\n",
      "play\n",
      "by\n",
      "you\n",
      ".\n",
      "I\n",
      "hope\n",
      "to\n",
      "play\n",
      "with\n",
      "you\n",
      "again\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\"Primo token: %s\" % doc[0].text) #with .text, we get the same as call above but as a string\n",
    "print(\"Ultimo token: %s\" % doc[-1].text)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGev04zk_t8I"
   },
   "source": [
    "Possiamo accedere alle singoli frasi con l'attributo *.sents*.\n",
    "<br>\n",
    "**NOTA BENE:** **.sents** è un generatore, il chè è un'ottima cosa nel caso di testi molto lunghi, per accedere alla singola frase tramite indexing dobbiamo convertirlo in una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Rswi-e8v_2Om",
    "outputId": "f3744e28-5901-4a7d-860d-303746269d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's was a great play by you.\n",
      "I hope to play with you again.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "That's was a great play by you."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bghHedW5wx3I"
   },
   "source": [
    "## Lemmatizzazione\n",
    "Possiamo accedere al lemma di ogni parola tramite l'attributo **.lemma_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "G7pBilBTw5AX",
    "outputId": "c83b438b-99f5-4b68-cc68-2e043d91c5f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tLEMMA\n",
      "That\t\tthat\n",
      "'s\t\tbe\n",
      "was\t\tbe\n",
      "a\t\ta\n",
      "great\t\tgreat\n",
      "play\t\tplay\n",
      "by\t\tby\n",
      "you\t\tyou\n",
      ".\t\t.\n",
      "I\t\tI\n",
      "hope\t\thope\n",
      "to\t\tto\n",
      "play\t\tplay\n",
      "with\t\twith\n",
      "you\t\tyou\n",
      "again\t\tagain\n",
      ".\t\t.\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN\\t\\tLEMMA\")\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\" % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V45mT4wmxG6h"
   },
   "source": [
    "**NOTA BENE**\n",
    "<br>\n",
    "Utilizzando l'attributo **.lemma**, quindi senza trattino basso (_), accediamo agli hash che codificano i lemma all'interno del dizionario di Spacy, non penso che tu avrai mai bisogno degli hash ma ti do questa informazione perché potresti scordarti di inserire il _ e non comprendere cosa sono tutti quei numeri che vengono fuori (esperienza personale :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "6kV10_71vcOc",
    "outputId": "fb4d449f-fb89-4039-86f7-5bed02603a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tLEMMA\t\tHASH\n",
      "That\t\tthat\t\t4380130941430378203\n",
      "'s\t\tbe\t\t10382539506755952630\n",
      "was\t\tbe\t\t10382539506755952630\n",
      "a\t\ta\t\t11901859001352538922\n",
      "great\t\tgreat\t\t8881679497796027013\n",
      "play\t\tplay\t\t8228585124152053988\n",
      "by\t\tby\t\t16764210730586636600\n",
      "you\t\t-PRON-\t\t561228191312463089\n",
      ".\t\t.\t\t12646065887601541794\n",
      "I\t\t-PRON-\t\t561228191312463089\n",
      "hope\t\thope\t\t4429974322456332988\n",
      "to\t\tto\t\t3791531372978436496\n",
      "play\t\tplay\t\t8228585124152053988\n",
      "with\t\twith\t\t12510949447758279278\n",
      "you\t\t-PRON-\t\t561228191312463089\n",
      "again\t\tagain\t\t4502205900248518970\n",
      ".\t\t.\t\t12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN\\t\\tLEMMA\\t\\tHASH\")\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\\t\\t%s\" % (token.text, token.lemma_, token.lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kX4RTKLPyTGM"
   },
   "source": [
    "## Stop words\n",
    "Anche Spacy ci mette a disposizione un'elenco di stop words, più corposo di quello di NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "E7NJG1zDxmhZ",
    "outputId": "3290a02f-ff5b-43fe-e4cd-61bacfc7a0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "Stop words totali: 326\n"
     ]
    }
   ],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(type(stopwords))\n",
    "\n",
    "print(\"Stop words totali: %d\" % len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylPkeldSyrUx"
   },
   "source": [
    "Le stop words vengono tornate all'interno di un set, che è un formato conveniente per eseguire operazioni di sottrazione tra insiemi, se vogliamo utilizzare l'indexing per stampare una parte del set dobbiamo convertirlo in una lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "tpKlr8VgypCd",
    "outputId": "45005cd5-9664-4514-b2fe-d401ce13b813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 stop words: ['three', 'yourself', 'they', 'again', 'such', 'then', 'so', 'an', 'everywhere', 'give']\n"
     ]
    }
   ],
   "source": [
    "print(\"Prime 10 stop words: %s\" % list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xwn-jr-NzJW7"
   },
   "source": [
    "Con Spacy non abbiamo bisogno di effettuare la rimozione delle stop words manualmente, dato che anche questo viene eseguito durante la creazione dell'oggetto *Doc*, possiamo vedere se un token è una stop words con l'attributo **is_stop**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "OhwU3Epgy7Cg",
    "outputId": "b1b38240-aaba-4022-d139-42732d7e3738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tIS STOP\n",
      "That\t\tTrue\n",
      "'s\t\tTrue\n",
      "was\t\tTrue\n",
      "a\t\tTrue\n",
      "great\t\tFalse\n",
      "play\t\tFalse\n",
      "by\t\tTrue\n",
      "you\t\tTrue\n",
      ".\t\tFalse\n",
      "I\t\tTrue\n",
      "hope\t\tFalse\n",
      "to\t\tTrue\n",
      "play\t\tFalse\n",
      "with\t\tTrue\n",
      "you\t\tTrue\n",
      "again\t\tTrue\n",
      ".\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN\\t\\tIS STOP\")\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\" % (token.text, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aFU8F8Szxm-"
   },
   "source": [
    "Quindi per rimuovere le stop words possiamo controllare tale attributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BEasenskzHqa",
    "outputId": "c394c420-a080-48bb-acb0-524c26f94c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[great, play, ., hope, play, .]\n"
     ]
    }
   ],
   "source": [
    "tokens_filtered = []\n",
    "\n",
    "for token in doc:\n",
    "  if(not token.is_stop):\n",
    "    tokens_filtered.append(token)\n",
    "\n",
    "print(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzZPEN7r0rQ-"
   },
   "source": [
    "## Un'esempio in Italiano\n",
    "Facciamo adesso un'esempio in italiano. Installiamo il modello per la lingua italiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "21xGYssE0-eP",
    "outputId": "98535640-3dca-4ce6-b033-ad1c0936e4be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.4.0/it_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.0 MB 526 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from it-core-news-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (1.20.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->it-core-news-sm==3.4.0) (1.1.1)\n",
      "Installing collected packages: it-core-news-sm\n",
      "Successfully installed it-core-news-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YmRWaFw1FII"
   },
   "source": [
    "e carichiamolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M3-f9goW0HmG",
    "outputId": "0e6a7d3e-676d-4755-eb02-604342d28dd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.it.Italian"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFDrDPoj1ISr"
   },
   "source": [
    "creiamo il documento e vediamo i tokens, i lemma e le stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "HJUit_tf052A",
    "outputId": "a5972244-74d4-4412-e39f-22c9b98390dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRASI\n",
      "[Oggi è una giornata afosa., Ho davvero voglia di una granita fresca]\n",
      "\n",
      "\n",
      "TOKEN\t\tLEMMA\t\tIS STOP\n",
      "Oggi\t\toggi\t\tTrue\n",
      "è\t\tessere\t\tTrue\n",
      "una\t\tuno\t\tTrue\n",
      "giornata\t\tgiornata\t\tFalse\n",
      "afosa\t\tafoso\t\tFalse\n",
      ".\t\t.\t\tFalse\n",
      "Ho\t\tavere\t\tTrue\n",
      "davvero\t\tdavvero\t\tFalse\n",
      "voglia\t\tvoglia\t\tFalse\n",
      "di\t\tdi\t\tTrue\n",
      "una\t\tuno\t\tTrue\n",
      "granita\t\tgranita\t\tFalse\n",
      "fresca\t\tfresco\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Oggi è una giornata afosa. Ho davvero voglia di una granita fresca\")\n",
    "\n",
    "print(\"FRASI\")\n",
    "\n",
    "\n",
    "print([sent for sent in doc.sents])\n",
    "print(\"\\n\") #spazio tra il print frasi e il seguente\n",
    "\n",
    "print(\"TOKEN\\t\\tLEMMA\\t\\tIS STOP\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\\t\\t%s\" % (token.text, token.lemma_, token.is_stop))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "text_preprocessing_spacy.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
