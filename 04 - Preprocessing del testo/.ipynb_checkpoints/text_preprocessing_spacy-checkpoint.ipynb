{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/4%20-%20Preprocessing%20del%20testo/text_preprocessing_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9lp6Cf0pa2M"
   },
   "source": [
    "# Preprocessing del testo con Spacy\n",
    "\n",
    "In questo notebook vedremo come possiamo eseguire la tokenizzazione, la rimozione delle stopwords e la lemmatizzazione con [Spacy](https://spacy.io/), una libreria Python per il natural language processing ad alte prestazioni progettata per essere utilizzata in produzione.\n",
    "Spacy adotta una politica del tipo \"pochi ma buoni\", cioè implementa solamente la tecnica più peformante per ogni tipo di operazione, per questo motivo spacy non implementa una funzione per lo stemming dato che queste solitamente portano a risultati più scarsi rispetto alla lemmatizzazione. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0MMYiLYt-kH"
   },
   "source": [
    "## Caricare un modello linguistico\n",
    "Per utilizzare Spacy dobbiamo prima scaricare e installare il modello per la lingua che vogliamo utilizzare, Spacy supporta oltre 49 lingue, [qui puoi trovare l'elenco completo con i relativi modelli](https://spacy.io/usage/models). Scarichiamo ed installiamo il modello per la lingua inglese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-gM65Yu2pWdB",
    "outputId": "8b1bf78d-5ca5-49f2-89c6-7ddb5cad2de6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZC5qfmCu0W_"
   },
   "source": [
    "L'output della funzione load è un'oggetto di tipo *Language*, che possiamo utilizzare per processare il nostro testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ry8GhJMAum-D",
    "outputId": "402bc671-6816-4c0e-b16c-210518de1f9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"That's was a great play by you. I hope to play with you again.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5Ej3mQ7vmWN"
   },
   "source": [
    "doc è un oggetto contentente il testo già processato, cosa vuol dire questo ? Che operazioni come tokenizzazione e lemmatizzazione sono già state eseguite e possiamo utilizzare degli appositi attributi per accedere ai risultati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqIaTSlyv7rf"
   },
   "source": [
    "## Tokenizzazione\n",
    "Possiamo accedere ai singoli token utilizzando l'attributo *.text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "jdpkGGOnwA0Y",
    "outputId": "12baea65-575a-4730-e66d-1e3cd32676ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primo token: That\n",
      "Ultimo token: .\n",
      "\n",
      "\n",
      "That\n",
      "'s\n",
      "was\n",
      "a\n",
      "great\n",
      "play\n",
      "by\n",
      "you\n",
      ".\n",
      "I\n",
      "hope\n",
      "to\n",
      "play\n",
      "with\n",
      "you\n",
      "again\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\"Primo token: %s\" % doc[0].text)\n",
    "print(\"Ultimo token: %s\" % doc[-1].text)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGev04zk_t8I"
   },
   "source": [
    "Possiamo accedere alle singoli frasi con l'attributo *.sents*.\n",
    "<br>\n",
    "**NOTA BENE:** *.sents* è un generatore, il chè è un'ottima cosa nel caso di testi molto lunghi, per accedere alla singola frase tramite indexing dobbiamo convertirlo in una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Rswi-e8v_2Om",
    "outputId": "f3744e28-5901-4a7d-860d-303746269d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's was a great play by you.\n",
      "I hope to play with you again.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bghHedW5wx3I"
   },
   "source": [
    "## Lemmatizzazione\n",
    "Possiamo accedere al lemma di ogni parola tramite l'attributo *.lemma_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "G7pBilBTw5AX",
    "outputId": "c83b438b-99f5-4b68-cc68-2e043d91c5f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tLEMMA\n",
      "That\t\tthat\n",
      "'s\t\tbe\n",
      "was\t\tbe\n",
      "a\t\ta\n",
      "great\t\tgreat\n",
      "play\t\tplay\n",
      "by\t\tby\n",
      "you\t\t-PRON-\n",
      ".\t\t.\n",
      "I\t\t-PRON-\n",
      "hope\t\thope\n",
      "to\t\tto\n",
      "play\t\tplay\n",
      "with\t\twith\n",
      "you\t\t-PRON-\n",
      "again\t\tagain\n",
      ".\t\t.\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN\\t\\tLEMMA\")\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\" % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V45mT4wmxG6h"
   },
   "source": [
    "**NOTA BENE**\n",
    "<br>\n",
    "Utilizzando l'attributo *.lemma*, quindi senza trattino basso (_), accediamo agli hash che codificano i lemma all'interno del dizionario di Spacy, non penso che tu avrai mai bisogno degli hash ma ti do questa informazione perché potresti scordarti di inserire il _ e non comprendere cosa sono tutti quei numeri che vengono fuori (esperienza personale :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "6kV10_71vcOc",
    "outputId": "fb4d449f-fb89-4039-86f7-5bed02603a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tLEMMA\t\tHASH\n",
      "That\t\tthat\t\t4380130941430378203\n",
      "'s\t\tbe\t\t10382539506755952630\n",
      "was\t\tbe\t\t10382539506755952630\n",
      "a\t\ta\t\t11901859001352538922\n",
      "great\t\tgreat\t\t8881679497796027013\n",
      "play\t\tplay\t\t8228585124152053988\n",
      "by\t\tby\t\t16764210730586636600\n",
      "you\t\t-PRON-\t\t561228191312463089\n",
      ".\t\t.\t\t12646065887601541794\n",
      "I\t\t-PRON-\t\t561228191312463089\n",
      "hope\t\thope\t\t4429974322456332988\n",
      "to\t\tto\t\t3791531372978436496\n",
      "play\t\tplay\t\t8228585124152053988\n",
      "with\t\twith\t\t12510949447758279278\n",
      "you\t\t-PRON-\t\t561228191312463089\n",
      "again\t\tagain\t\t4502205900248518970\n",
      ".\t\t.\t\t12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN\\t\\tLEMMA\\t\\tHASH\")\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\\t\\t%s\" % (token.text, token.lemma_, token.lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kX4RTKLPyTGM"
   },
   "source": [
    "## Stop words\n",
    "Anche Spacy ci mette a disposizione un'elenco di stop words, più corposo di quello di NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "E7NJG1zDxmhZ",
    "outputId": "3290a02f-ff5b-43fe-e4cd-61bacfc7a0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "Stop words totali: 305\n"
     ]
    }
   ],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(type(stopwords))\n",
    "\n",
    "print(\"Stop words totali: %d\" % len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylPkeldSyrUx"
   },
   "source": [
    "Le stop words vengono tornate all'interno di un set, che è un formato conveniente per eseguire operazioni di sottrazione tra insiemi, se vogliamo utilizzare l'indexing per stampare una parte del set dobbiamo convertirlo in una lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "tpKlr8VgypCd",
    "outputId": "45005cd5-9664-4514-b2fe-d401ce13b813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 stop words: ['though', 'without', 'ever', 'wherever', 'own', 'who', 'always', 'become', 'something', 'seeming']\n"
     ]
    }
   ],
   "source": [
    "print(\"Prime 10 stop words: %s\" % list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xwn-jr-NzJW7"
   },
   "source": [
    "Con Spacy non abbiamo bisogno di effettuare la rimozione delle stop words manualmente, dato che anche questo viene eseguito durante la creazione dell'oggetto *Doc*, possiamo vedere se un token è una stop words con l'attributo *is_stop*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "OhwU3Epgy7Cg",
    "outputId": "b1b38240-aaba-4022-d139-42732d7e3738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tIS STOP\n",
      "That\t\tFalse\n",
      "'s\t\tFalse\n",
      "was\t\tTrue\n",
      "a\t\tTrue\n",
      "great\t\tFalse\n",
      "play\t\tFalse\n",
      "by\t\tTrue\n",
      "you\t\tTrue\n",
      ".\t\tFalse\n",
      "I\t\tFalse\n",
      "hope\t\tFalse\n",
      "to\t\tTrue\n",
      "play\t\tFalse\n",
      "with\t\tTrue\n",
      "you\t\tTrue\n",
      "again\t\tTrue\n",
      ".\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKEN\\t\\tIS STOP\")\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\" % (token.text, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aFU8F8Szxm-"
   },
   "source": [
    "Quindi per rimuovere le stop words possiamo controllare tale attributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BEasenskzHqa",
    "outputId": "c394c420-a080-48bb-acb0-524c26f94c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[That, 's, great, play, ., I, hope, play, .]\n"
     ]
    }
   ],
   "source": [
    "tokens_filtered = []\n",
    "\n",
    "for token in doc:\n",
    "  if(not token.is_stop):\n",
    "    tokens_filtered.append(token)\n",
    "\n",
    "print(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzZPEN7r0rQ-"
   },
   "source": [
    "## Un'esempio in Italiano\n",
    "Facciamo adesso un'esempio in italiano. Installiamo il modello per la lingua italiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "21xGYssE0-eP",
    "outputId": "98535640-3dca-4ce6-b033-ad1c0936e4be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.0.0/it_core_news_sm-2.0.0.tar.gz#egg=it_core_news_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.0.0/it_core_news_sm-2.0.0.tar.gz (36.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 36.5MB 8.0MB/s \n",
      "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
      "  Running setup.py install for it-core-news-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed it-core-news-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/it_core_news_sm -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/it_core_news_sm\n",
      "\n",
      "    You can now load the model via spacy.load('it_core_news_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YmRWaFw1FII"
   },
   "source": [
    "e carichiamolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M3-f9goW0HmG",
    "outputId": "0e6a7d3e-676d-4755-eb02-604342d28dd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.it.Italian"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFDrDPoj1ISr"
   },
   "source": [
    "creiamo il documento e vediamo i tokens, i lemma e le stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "HJUit_tf052A",
    "outputId": "a5972244-74d4-4412-e39f-22c9b98390dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRASI\n",
      "[Oggi è una giornata afosa., Ho davvero voglia di una granita fresca]\n",
      "\n",
      "\n",
      "TOKEN\t\tLEMMA\t\tIS STOP\n",
      "Oggi\t\tOggi\t\tFalse\n",
      "è\t\tessere\t\tFalse\n",
      "una\t\tuna\t\tTrue\n",
      "giornata\t\tgiornata\t\tFalse\n",
      "afosa\t\tafoso\t\tFalse\n",
      ".\t\t.\t\tFalse\n",
      "Ho\t\tHo\t\tFalse\n",
      "davvero\t\tdavvero\t\tFalse\n",
      "voglia\t\tvolere\t\tFalse\n",
      "di\t\tdi\t\tTrue\n",
      "una\t\tuna\t\tTrue\n",
      "granita\t\tgranire\t\tFalse\n",
      "fresca\t\tfresco\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Oggi è una giornata afosa. Ho davvero voglia di una granita fresca\")\n",
    "\n",
    "print(\"FRASI\")\n",
    "\n",
    "\n",
    "print([sent for sent in doc.sents])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"TOKEN\\t\\tLEMMA\\t\\tIS STOP\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "  print(\"%s\\t\\t%s\\t\\t%s\" % (token.text, token.lemma_, token.is_stop))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "text_preprocessing_spacy.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
