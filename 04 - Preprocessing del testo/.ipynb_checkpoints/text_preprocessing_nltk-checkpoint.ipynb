{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/4%20-%20Preprocessing%20del%20testo/text_preprocessing_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIngrrjZR0C-"
   },
   "source": [
    "# Tokenizzazione\n",
    "La tokenizzazione è il processo di estrazione delle parti costituenti del testo, chiamati per l'appunto **tokens**, in sostanza i tokens sono le parole e i caratteri di punteggiatura che compongono il testo.\n",
    "<br>\n",
    "Vediamo come eseguire la tokenizzazione con nltk, importiamo il modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKp4HdRwP9Il"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mG2uOccZsIg6"
   },
   "source": [
    "Per eseguire la tokenizzazione, dobbiamo scaricare il pacchetto di dati 'punkt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "trbKIsjMRbXy",
    "outputId": "7e2d3f90-356a-49ef-ab50-b9bfd8d85ac5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adrianacuppuleri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUZubpMOspJv"
   },
   "source": [
    "Per eseguire la tokenizzazione possiamo usare la funzione *word_tokenizer(sentence)* di nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qbkzAN_4Rkpk",
    "outputId": "41e95ff8-b34f-4bc6-d2b4-cc82f66e6397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cos', \"'\", 'è', 'questa', 'fretta', '?', 'Facciamolo', \"un'altra\", 'volta', ',', 'ti', 'va', 'bene', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Cos'è questa fretta? Facciamolo un'altra volta, ti va bene?\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ns0A9_sPtf4g"
   },
   "source": [
    "Come vedi il risultato è una lista dei tokens, cioè di parole e punteggiatura che costituiscono la frase, ma non avremmo ottenuto lo stesso risultato usando il metodo *split()* ? Vediamolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "Dp_MmD2Ttzrc",
    "outputId": "5ff86770-497d-40c4-cac9-63aafeacbb5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Cos'è\",\n",
       " 'questa',\n",
       " 'fretta?',\n",
       " 'Facciamolo',\n",
       " \"un'altra\",\n",
       " 'volta,',\n",
       " 'ti',\n",
       " 'va',\n",
       " 'bene?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Cos'è questa fretta? Facciamolo un'altra volta, ti va bene?\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrs0h3YGt7Ko"
   },
   "source": [
    "Come vedi il risultato non è lo stesso, il metodo *split()* semplicemente separa le parole in base agli spazi, quindi la punteggiatura non viene isolata.\n",
    "<br>\n",
    "Oltre a questo la tokenizzazione ci dovrebbe permette di dividere parole unite in forma contratta (Cos'è = Cosa+è). Purtroppo la funzione *word_tokenizer* non supporta la lingua italiana, facciamo una prova con l'inglese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mUeC0RIpQAA7",
    "outputId": "9bcdacf0-cc8f-4e6d-fc1f-85452a648814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'d\", 'love', 'to', 'visit', 'U.S.A', ',', 'but', 'I', 'ca', \"n't\", 'afford', 'it', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"I'd love to visit U.S.A, but I can't afford it!\" # Mi piacerebbe visitare gli Stati Uniti ma non posso permettermelo\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoWF_gBJv5FA"
   },
   "source": [
    "Come vedi, con la lingua inglese, la funzione *word_tokenizer* è riuscita a separare anche le parole unite in forma contratta, nota anche come è riuscita a comprendere che U.S.A è un'unica parola.\n",
    "<br>\n",
    "Possiamo tokenizzare anche intere frasi piuttosto che singole parole, per farlo dobbiamo usare la funzione *sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AfCmFAbC-7hT",
    "outputId": "319fb284-3ee8-4ff1-dbfa-4b38d6d12537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'd love to visit U.S.A, but I can't afford it.\", \"Instead this summer I'll visit U.K\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I'd love to visit U.S.A, but I can't afford it. Instead this summer I'll visit U.K\" # Mi piacerebbe visitare gli Stati Uniti ma non posso permettermelo. Invece questa estate visiterò il Regno Unito. \n",
    "\n",
    "tokens_sent = sent_tokenize(text)\n",
    "\n",
    "print(tokens_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ava0Sk6yZb6D"
   },
   "source": [
    "# Stop Words\n",
    "Le stop words sono quelle **parole comuni del linguaggio che portano poca o nessuna informazione al discorso**, esempi possono essere le congiunzioni o verbi comuni come il verbo essere e il verbo avere.\n",
    "<br>\n",
    "NLTK contiene liste di stop words per diversi linguaggi, per ottenerle dobbiamo prima scaricare il corpus 'stopwords'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "MZtHEWhcaEV2",
    "outputId": "e976016a-caf7-44b0-92fc-f930f3320465"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adrianacuppuleri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPyGJZn1arEG"
   },
   "source": [
    "Poi possiamo ottenere una lista per una determinata lingua utilizzando la funzione *words* del metodo *stopwords*, indicando al suo interno la lingua per la quale vogliamo avere le stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1XdTxFy3Z5lx",
    "outputId": "93b0affe-adc1-4ea4-998c-a6f5d057cde0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words totali: 179\n",
      "Prime 10 stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "print(\"Stop words totali: %d\" % len(en_stopwords))\n",
    "print(\"Prime 10 stop words: %s\" % en_stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phitv0LTa8Ih"
   },
   "source": [
    "Come vedi per l'inglese abbiamo 179 stop words, vediamo per la lingua italiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EjNIQLLFafK_",
    "outputId": "4702cb0c-0955-4b12-a413-cda569c236e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words totali: 279\n",
      "Prime 10 stop words: ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n"
     ]
    }
   ],
   "source": [
    "it_stopwords = stopwords.words('italian')\n",
    "\n",
    "print(\"Stop words totali: %d\" % len(it_stopwords))\n",
    "print(\"Prime 10 stop words: %s\" % it_stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LP7X7neAbEL4"
   },
   "source": [
    "Per l'italiano abbiamo 279 parole, possiamo rimuovere le stop words dalla nostra lista di tokens iterando sopra di essi e verificando se il token è presente o meno nella lista di stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "WHHXpHzWboJe",
    "outputId": "3a7c5705-3592-4783-e08c-c4d16a836a6f"
   },
   "outputs": [],
   "source": [
    "text = \"Io sono una persona simpatica, solo che non mi piacciono gli esseri umani, preferisco i gatti\"\n",
    "\n",
    "it_stopwords = stopwords.words('italian')\n",
    "\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords: ['Io', 'sono', 'una', 'che', 'non', 'mi', 'gli', 'i']\n",
      "Tokens rimasti: ['persona', 'simpatica', ',', 'solo', 'piacciono', 'esseri', 'umani', ',', 'preferisco', 'gatti']\n"
     ]
    }
   ],
   "source": [
    "tokens_filtered = []\n",
    "tokens_stopwords = []\n",
    "\n",
    "for token in tokens:\n",
    "    if(token.lower() not in it_stopwords):\n",
    "        tokens_filtered.append(token)\n",
    "    else:\n",
    "        tokens_stopwords.append(token)\n",
    "    \n",
    "    \n",
    "print(\"stopwords: %s\" % tokens_stopwords)\n",
    "print(\"Tokens rimasti: %s\" % tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hONBoj60cldY"
   },
   "source": [
    "Come vedi abbiamo convertito la parola in minuscolo per assicurarsi il matching anche delle parole a inizio frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiOr2eubR3MF"
   },
   "source": [
    "# Stemming\n",
    "Lo stemming è un processo che consiste nel ridurre le parole di un testo nella loro forma base, chiamata appunto **stem**.\n",
    "<br><br>\n",
    "NLTK ci mette a disposizione due oggetti per eseguire lo stemming, il PorterStemmer e lo SnowballStemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JzMDMKHC4L5q"
   },
   "source": [
    "## Porter Stemmer\n",
    "Il Poter Stemmer è un'algoritmo di stemming che applica una serie di regole in 5 steps per rimuovere il suffisso dalla frase.\n",
    "<br>\n",
    "Importiamo la classe *PorterStemmer* da nltk e testiamola su una frase, questo algoritmo funziona solo con la lingua inglese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "LVEeEfNUQFzp",
    "outputId": "9cfeb4c7-978c-4022-e533-7ca7c7d4d90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tSTEM\n",
      "My\t\tmy\n",
      "cats\t\tcat\n",
      "ate\t\tate\n",
      "some\t\tsome\n",
      "mice\t\tmice\n",
      "while\t\twhile\n",
      "I\t\ti\n",
      "was\t\twa\n",
      "playing\t\tplay\n",
      "King\t\tking\n",
      "of\t\tof\n",
      "the\t\tthe\n",
      "Riddles\t\triddl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# lo stemmer prende in input la singola parola\n",
    "# quindi passiamo un token per volta\n",
    "# utilizzando un ciclo for\n",
    "\n",
    "print(\"TOKEN\\t\\tSTEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test porter stemmer for plurals\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', 'siezing', 'itemization','sensational', 'traditional', 'reference', 'colonizer','plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = [stemmer.stem(plural) for plural in plurals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rc2Yw_kf7VYq"
   },
   "source": [
    "## Snowball Stemmer\n",
    "Snowball è un'altro algoritmo di stemming che apporta alcune migliorie al Porter Stemmer e a differenza di questo supporta diverse lingue con NLTK, incluso l'italiano.\n",
    "<br>\n",
    "Possiamo passare la lingua come parametro dello stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "JHd6iP4ESa5w",
    "outputId": "8688c0ca-fb35-4322-c50c-33e79b742b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tSTEM\n",
      "My\t\tmy\n",
      "cats\t\tcat\n",
      "ate\t\tate\n",
      "some\t\tsome\n",
      "mice\t\tmice\n",
      "while\t\twhile\n",
      "I\t\ti\n",
      "was\t\twas\n",
      "playing\t\tplay\n",
      "King\t\tking\n",
      "of\t\tof\n",
      "the\t\tthe\n",
      "Riddles\t\triddl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "print(\"TOKEN\\t\\tSTEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "8MeHyejFTuea",
    "outputId": "34d86eb4-ec27-434e-b0e4-9a1a1e2c7341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tSTEM\n",
      "I\t\ti\n",
      "miei\t\tmie\n",
      "gatti\t\tgatt\n",
      "hanno\t\thann\n",
      "mangiato\t\tmang\n",
      "alcuni\t\talcun\n",
      "topi\t\ttop\n",
      "mentre\t\tmentr\n",
      "io\t\tio\n",
      "stavo\t\tstav\n",
      "giocavo\t\tgioc\n",
      "a\t\ta\n",
      "il\t\til\n",
      "signore\t\tsignor\n",
      "degli\t\tdegl\n",
      "enigmi\t\tenigm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "text = \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "\n",
    "print(\"TOKEN\\t\\tSTEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tSTEM\n",
      "I\t\ti\n",
      "miei\t\tmiei\n",
      "gatti\t\tgatt\n",
      "hanno\t\thanno\n",
      "mangiato\t\tmang\n",
      "alcuni\t\talcun\n",
      "topi\t\ttop\n",
      "mentre\t\tmentr\n",
      "io\t\tio\n",
      "stavo\t\tstavo\n",
      "giocavo\t\tgioc\n",
      "a\t\ta\n",
      "il\t\til\n",
      "signore\t\tsignor\n",
      "degli\t\tdegli\n",
      "enigmi\t\tenigm\n"
     ]
    }
   ],
   "source": [
    "#not stemming stop words in italian\n",
    "text = \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer(\"italian\", ignore_stopwords=True)\n",
    "\n",
    "print(\"TOKEN\\t\\tSTEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entrat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stem a word\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "stemmer.stem(\"entrato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "having\n"
     ]
    }
   ],
   "source": [
    "#decide not to stem stopwords\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "print(stemmer.stem(\"having\"))\n",
    "print(stemmer2.stem(\"having\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "En69Ke2j8Djh"
   },
   "source": [
    "Ovviamente la precisione dell'algoritmo è maggiore per la lingua inglese che per la lingua italiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5PFDdUHPzQk"
   },
   "source": [
    "## Lancaster Stemmer\n",
    "Il Lancaster è un'altro algoritmo di stemming che applica regole più aggressive per la riduzione delle parole, anche eccessivamente aggressive, facciamo un esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "VWA20mfXP1ie",
    "outputId": "0b0d4834-88a2-433e-c958-c67d6e42051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tSTEM\n",
      "My\t\tmy\n",
      "cats\t\tcat\n",
      "ate\t\tat\n",
      "some\t\tsom\n",
      "mice\t\tmic\n",
      "while\t\twhil\n",
      "I\t\ti\n",
      "was\t\twas\n",
      "playing\t\tplay\n",
      "King\t\tking\n",
      "of\t\tof\n",
      "the\t\tthe\n",
      "Riddles\t\triddl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stemmer=LancasterStemmer()\n",
    "\n",
    "print(\"TOKEN\\t\\tSTEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyOr_FaxQaVw"
   },
   "source": [
    "Come vedi usando il Lancaster alcune parole sono state tagliate. Nella pratica possiamo adottare tranquillamente lo Snowball, il quale dovrebbe portare a risultati più stabili e precisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUrgr8xbErxS"
   },
   "source": [
    "# Lemmatizatione\n",
    "La lemmatizzazione ci permette di ridurre una parola dalla sua forma flessa alla sua forma canonica, detta appunto **lemma**, il quale a differenza dello stem è una parola reale.\n",
    "Possiamo eseguire la lemmatizzazione con nltk usando il *WordNetLemmatizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iZ_qaI2yMp37",
    "outputId": "70b06b19-449e-4b10-b691-12c22e94f9d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adrianacuppuleri/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "u8siCFEnEqmB",
    "outputId": "92cddd78-68fd-4490-a569-f13882513d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tLEM\n",
      "My\t\tMy\n",
      "cats\t\tcat\n",
      "ate\t\tate\n",
      "some\t\tsome\n",
      "mice\t\tmouse\n",
      "while\t\twhile\n",
      "I\t\tI\n",
      "was\t\twa\n",
      "playing\t\tplaying\n",
      "King\t\tKing\n",
      "of\t\tof\n",
      "the\t\tthe\n",
      "Riddles\t\tRiddles\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"TOKEN\\t\\tLEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token, lemmatizer.lemmatize(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFXy2CVOXD82"
   },
   "source": [
    "Come vedi a lemmatizzato perfettamente i nomi (mice -> mouse), però non ha fatto altrettanto con i verbi, perchè ? Perché dobbiamo anche specificare il tipo di parola utilizzando il parametro pos, se non lo facciamo crederà che si tratti di un nome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "i3wH36NLMfhB",
    "outputId": "220dfa2a-3d7f-4eec-e186-17391408694f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t\tLEM\n",
      "My\t\tMy\n",
      "cats\t\tcat\n",
      "ate\t\teat\n",
      "some\t\tsome\n",
      "mice\t\tmouse\n",
      "while\t\twhile\n",
      "I\t\tI\n",
      "was\t\tbe\n",
      "playing\t\tplay\n",
      "King\t\tKing\n",
      "of\t\tof\n",
      "the\t\tthe\n",
      "Riddles\t\tRiddles\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = [(\"My\",\"n\"),(\"cats\",\"n\"),(\"ate\",\"v\"), (\"some\",\"n\"), (\"mice\",\"n\"), (\"while\",\"r\"), (\"I\",\"n\"), (\"was\",\"v\"), (\"playing\",\"v\"), (\"King\",\"v\"), (\"of\",\"n\"),(\"the\",\"n\"),(\"Riddles\",\"n\")]\n",
    "\n",
    "print(\"TOKEN\\t\\tLEM\")\n",
    "\n",
    "for token in tokens:\n",
    "  print(\"%s\\t\\t%s\" % (token[0], lemmatizer.lemmatize(token[0], pos=token[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1IaEPSmCY0n6"
   },
   "source": [
    "Pos sta per part-of-speech, cioè parte del discorso, è possibile automatizzare l'identificazione della parte del discorso in modo da non doverli inserire manualmente, nella prossima sezione vedremo come."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "text_preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
